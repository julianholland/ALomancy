{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77257188",
   "metadata": {},
   "source": [
    "# Creating a Standard AL workflow for water\n",
    "\n",
    "For this tutorial we will\n",
    "- create an initial starting data set for water\n",
    "- set up input yaml files for an `alomancy` Standard AL Workflow object call\n",
    "- run a lightweight local version of the code to perform the training\n",
    "## The `StandardActiveLearningWorkflow` Object\n",
    "All workflow objects are a subset of the `BaseActiveLearningWorkflow` object which essential performs the following loop\n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A((Initial Dataset)) --> B([Train MLIP Committee])\n",
    "    B --> C([Structure Generation])\n",
    "    C --> D[Uncertainty-based Selection]\n",
    "    D --> E([High-Accuracy Evaluation])\n",
    "    E --> F[Update Training Dataset]\n",
    "    F --> B\n",
    "\n",
    "    style A fill:#e66027\n",
    "    style B fill:#e87322\n",
    "    style C fill:#eb861e\n",
    "    style D fill:#efa119\n",
    "    style E fill:#708e4c\n",
    "    style F fill:#328566\n",
    "```\n",
    "\n",
    "Where the round-edged nodes are required to be defiend by a subclass. \n",
    "\n",
    "The `StandardActiveLearningWorkflow` defines them as\n",
    "\n",
    "- **Train MLIP Committee**: Creates a committee of from scratch [MACE](https://mace-docs.readthedocs.io/en/latest/index.html) models\n",
    "- **Structure Generation**: Selects high standard deviation structures from a user defined number of [Langevin molecular dynamics (MD)](https://ase-lib.org/ase/md.html#module-ase.md.langevin) by using the committee of models\n",
    "- **High-Accuracy Evaluation**: Evaluates the structuers using the plane wave density functional theory (DFT) software [Quantum Espresso (QE)](https://www.quantum-espresso.org)\n",
    "\n",
    "With the other nodes in the flowchart being globally defined and therefore untouched here. \n",
    "\n",
    "Changing these nodes is covered in the next tutorial. For now we shall leave them as the `StandardActiveLearningWorkflow` defaults.\n",
    "\n",
    "## Creating a Dataset\n",
    "It is important that our initial data set contains\n",
    "- All types of atoms that we wish to train upon (H and O)\n",
    "- A diverse collection of potential stoichiometries, atom numbers, and cell densities\n",
    "\n",
    "We will start with constructing atoms objects for the indiviual atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea44dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.calculators.emt import EMT\n",
    "\n",
    "isolated_atoms_list=[]\n",
    "\n",
    "# construct single atom Atoms objects\n",
    "h_atom = Atoms('H', positions=[[0,0,0]], cell=[10,10,10], pbc=True)\n",
    "o_atom = Atoms('O', positions=[[0,0,0]], cell=[10,10,10], pbc=True)\n",
    "\n",
    "# Evaluate the objects energetically, in reality you should use a much higher accuracy method than EMT such as DFT\n",
    "for atom in [h_atom, o_atom]:\n",
    "    atom.calc=EMT()\n",
    "    atom.get_potential_energy()\n",
    "    atom.info['config_type']='IsolatedAtom'\n",
    "    isolated_atoms_list.append(atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad66c5b",
   "metadata": {},
   "source": [
    "We assign config types to assist with the MACE training later on.\n",
    "\n",
    "Next, we will add molecules to our data set. We will also add deformed molecules to increase the amount of information in our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c14dc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.build import molecule\n",
    "from ase.optimize import BFGS\n",
    "\n",
    "# create some locally optimisedsingle molecule objects\n",
    "single_molecule_list=[]\n",
    "molecule_type_list=['H2', 'O2', 'H2O', 'H2O2', 'O3']\n",
    "\n",
    "for mol in molecule_type_list:\n",
    "    mol = molecule(mol)\n",
    "    mol.cell = [10, 10, 10]\n",
    "    mol.pbc = True\n",
    "    mol.calc = EMT()\n",
    "    opt = BFGS(mol, logfile=None, trajectory=None)\n",
    "    opt.run(fmax=0.05)\n",
    "    mol.info['config_type'] = 'StandardMolecule'\n",
    "    single_molecule_list.append(mol)\n",
    "\n",
    "\n",
    "# define the deformation parameters\n",
    "max_contraction = 0.3\n",
    "max_expansion = 1.5\n",
    "deformation_samples = 10\n",
    "\n",
    "# create deformation multipliers\n",
    "position_deformation_multipliers = [1 - max_contraction + (max_expansion - (1 - max_contraction)) * i / (deformation_samples - 1) for i in range(deformation_samples)]\n",
    "\n",
    "# create deformed molecules\n",
    "deformed_single_molecule_list = []\n",
    "for mol in single_molecule_list:\n",
    "    for multiplier in position_deformation_multipliers:\n",
    "        deformed_mol = mol.copy()\n",
    "        deformed_mol.set_positions(mol.get_positions() * multiplier)\n",
    "        deformed_mol.calc = EMT()\n",
    "        deformed_mol.get_potential_energy()\n",
    "        deformed_mol.info['config_type'] = 'DeformedMolecule'\n",
    "        deformed_single_molecule_list.append(deformed_mol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fcb5b",
   "metadata": {},
   "source": [
    "Finally, lets generate some randomised molecular clusters so our model will know how to interpret multiple molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e50a946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_number_of_molecules = 10\n",
    "min_number_of_molecules = 2\n",
    "\n",
    "random_cluster_list = []\n",
    "cell_size = 20\n",
    "# generate random clusters of molecules\n",
    "for i in range(100):\n",
    "    number_of_molecules = np.random.randint(min_number_of_molecules, max_number_of_molecules + 1)\n",
    "    selected_molecule_array = np.random.choice(len(single_molecule_list), size=number_of_molecules, replace=True)\n",
    "    selected_molecules = [single_molecule_list[i] for i in selected_molecule_array]\n",
    "\n",
    "    cluster = Atoms(cell=[cell_size, cell_size, cell_size], pbc=True)\n",
    "    for mol in selected_molecules:\n",
    "        mol_copy = Atoms(mol.symbols, positions=mol.positions)\n",
    "        # random translation\n",
    "        mol_copy.set_positions(mol.positions + np.random.rand(3) * len(mol.get_positions()) * cell_size)\n",
    "        # random rotation\n",
    "        mol_copy.rotate(np.random.rand() * 360, 'z', center='COM')\n",
    "        mol_copy.rotate(np.random.rand() * 360, 'y', center='COM')\n",
    "        mol_copy.rotate(np.random.rand() * 360, 'x', center='COM')\n",
    "        cluster += mol_copy\n",
    "\n",
    "    cluster.calc = EMT()\n",
    "    opt = BFGS(cluster, logfile=None, trajectory=None)\n",
    "    opt.run(fmax=0.05)\n",
    "    cluster.info['config_type'] = 'RandomCluster'\n",
    "    cluster.info['number_of_molecules'] = number_of_molecules\n",
    "    cluster.info['molecule_types'] = [mol.info['config_type'] for mol in selected_molecules]\n",
    "    random_cluster_list.append(cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97711d1",
   "metadata": {},
   "source": [
    "We have now generated all structures for our initial dataset lets combine everything, then split them into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b58fdbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total structures: 157\n",
      "Training set size: 125\n",
      "Test set size: 32\n"
     ]
    }
   ],
   "source": [
    "all_structures = isolated_atoms_list + single_molecule_list + deformed_single_molecule_list + random_cluster_list\n",
    "\n",
    "# assign REF_energy and REF_forces to each structure to make it easier for MACE to find\n",
    "for structure in all_structures:\n",
    "    structure.info['REF_energy'] = structure.get_potential_energy()\n",
    "    structure.arrays['REF_forces'] = structure.get_forces()\n",
    "\n",
    "test_train_split = int(len(all_structures) * 0.8)\n",
    "train_set = all_structures[:test_train_split]\n",
    "test_set = all_structures[test_train_split:]\n",
    "\n",
    "print(f\"Total structures: {len(all_structures)}\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30c367",
   "metadata": {},
   "source": [
    "This is a relatively small dataset but will suit us for our purposes for now.\n",
    "\n",
    "\n",
    "## Setting-up the `ALomancy` `StandardActiveLearningWorkflow` Input\n",
    "\n",
    "Now that we have the training data we can start setting up the input\n",
    "\n",
    "We will use the `StandardActiveLearningWorkflow` for this. Customizing your workflow to your needs will be covered in later tutorials. \n",
    "\n",
    "The `StandardActiveLearningWorkflow` object takes 5 inputs\n",
    "\n",
    "- `initial_train_file_path`: A string of the file path for the training file\n",
    "- `initial_test_file_path`: A string of the file path for the test file\n",
    "- `jobs_dict`: A dictionary defining all settings for the subclasses inside `StandardActiveLearningWorkflow`\n",
    "- `number_of_al_loops`: The number of active learning loops one wishes to perform\n",
    "- `verbose`: An integer defining how much the program talks to you (currently only 0 or 1)\n",
    "\n",
    "As we already have made our data set in the previous version lets first write these to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0897eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write\n",
    "\n",
    "test_set_path= 'test_set.xyz'\n",
    "train_set_path = 'train_set.xyz'\n",
    "\n",
    "write(train_set_path, train_set, format='extxyz')\n",
    "write(test_set_path, test_set, format='extxyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f76d00",
   "metadata": {},
   "source": [
    "Next we will need to define our `jobs_dict`. This can be quite complicated as it needs to contain a lot of information. The structure of `jobs_dict` can be broken down into the **three** types of modules.\n",
    "\n",
    "- `mlip_committee`: where the MLIP is trained. For the uneditted `StandardActiveLearningWorkflow` this also contains the MACE settings as `mace_fit_kwargs`\n",
    "    - `name`: name of your MLIP committee job\n",
    "    - `size_of_committee`: number of MLIP models to train\n",
    "    - `max_time`: string of the maximum time allowed for each element in this step\n",
    "    - `hpc` the high performancy computer settings associated with this calculation\n",
    "\n",
    "- `structure_generation`: where the structures for training are generated. For the uneditted `StandardActiveLearningWorkflow` this also contains the settings for the MD as `run_md_kwargs`\n",
    "    - `name`: name of your structure generation job\n",
    "    - `desired_number_of_structures`: number of structures to generate\n",
    "    - `max_time`: string of the maximum time allowed for each element in this step\n",
    "    - `structure_selection_kwargs`: settings to decide how to select structures \n",
    "        - `max_number_of_concurrent_jobs`: number of structure generation jobs to perform (attempted concurrently depending on resources)\n",
    "        - `chem_formula_list`: list of chemical formula allowed to be used in the structure selection, if all allowed set to none\n",
    "        - `atom_number_range`: tuple defining the minimum and maximum numbers of atoms allowed to be used in the structure generation jobs\n",
    "        - `enforce_chemical_diversity`: boolean that will ensure that no two same chemical formulas are chosen to be used in the structure generation jobs, if possible\n",
    "    - `hpc` the high performancy computer settings associated with this calculation\n",
    "\n",
    "- `high_accuracy_evaluation`: where the selected structures' energy and forces are calcualted. For the uneditted `StandardActiveLearningWorkflow` this contains the settings for the QE DFT calcualtions as `qe_input_kwargs`\n",
    "    - `name`: name of your high accuracy evaluation job\n",
    "    - `max_time`: string of the maximum time allowed for each element in this step\n",
    "    - `hpc` the high performancy computer settings associated with this calculation\n",
    "\n",
    "Each of these modules will need an associated high performance computer (HPC) dictionary. This dictionary defines \n",
    "\n",
    "- `hpc_name`: string of the ssh name of your hpc, this should be the same as the one in your `.expyre/config.json` file\n",
    "- `gpu`: boolean for whether the associated submission has access to a GPU processor\n",
    "- `pre_cmds`: list of commands to run before running the job, typically thesee involve activating the correct python environment\n",
    "- `partitions`: list of one, partition you wish to use in your submission\n",
    "- `node_info`: information about the nodes in the HPC \n",
    "    - `ranks_per_system`: number of MPIs to associate with each calculation\n",
    "    - `ranks_per_node`: number of MPIs available in each node\n",
    "    - `threads_per_rank`: number of OMP threads to use for each MPI\n",
    "    - `max_mem_per_node`: string defining the memory to use for each node\n",
    "- `high_accuracy_executable_path`: string of the path to your executable. Only needed for HPCs used for your `high_accuracy_evaluation` module and that require the calling of an executable\n",
    "- `pp_path`: string of the path to your pseudopotentials directory. Only needed for HPCs used for your `high_accuracy_evaluation` module and that require pseudopotetials\n",
    "- `pseudo_dict`: dictionary of elements present in the generated structures with their respective pseudopotential in the psuedopotentials directory defined by `pp_path`. Only needed for HPCs used for your `high_accuracy_evaluation` module and that require pseudopotetials\n",
    "\n",
    "\n",
    "As these are quite large dictionaries it is often easier to save these settings as two configuration files either as yaml or json\n",
    "\n",
    "Firstly, lets do the main dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33db389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrogen energy: 3.210 eV\n",
      "Oxygen energy: 4.600 eV\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# firstly we need the energy values of the isolated atoms to more accurately fit the model\n",
    "h_energy=isolated_atoms_list[0].get_potential_energy()\n",
    "o_energy=isolated_atoms_list[1].get_potential_energy()\n",
    "\n",
    "print(f\"Hydrogen energy: {h_energy:.3f} eV\")\n",
    "print(f\"Oxygen energy: {o_energy:.3f} eV\")\n",
    "standard_water_config = {\n",
    "  'mlip_committee': {\n",
    "    'name': 'mace_committee',\n",
    "    'size_of_committee': 3,\n",
    "    'max_time': '5H',\n",
    "    'mace_fit_kwargs': {\n",
    "      'E0s': {\n",
    "        1: float(h_energy),\n",
    "        8: float(o_energy),\n",
    "      },\n",
    "      'atomic_numbers': [1, 8],\n",
    "      'energy_key': \"REF_energy\",\n",
    "      'forces_key': \"REF_forces\",\n",
    "    },\n",
    "    'hpc' : 'your_hpc_dict_0'\n",
    "  },\n",
    "  'structure_generation': {\n",
    "    'name': 'md_1200_generation',\n",
    "    'desired_number_of_structures': 50,\n",
    "    'max_time': \"10H\",\n",
    "    'structure_selection_kwargs': {\n",
    "      'max_number_of_concurrent_jobs': 5,\n",
    "      'chem_formula_list': None,\n",
    "      'atom_number_range': [9, 21],\n",
    "      'enforce_chemical_diversity': True,\n",
    "    },\n",
    "    'run_md_kwargs': {\n",
    "      'steps': 20000,\n",
    "      'temperature': 1200,\n",
    "      'timestep_fs': 0.5,\n",
    "      'friction': 0.002,\n",
    "    },\n",
    "    'hpc': 'your_hpc_dict_0',\n",
    "  },\n",
    "  'high_accuracy_evaluation': {\n",
    "    'name': 'qe_dft',\n",
    "    'max_time': \"30m\",\n",
    "    'qe_input_kwargs': {\n",
    "        'control': {\n",
    "            'verbosity': \"high\",\n",
    "            'prefix': \"qe\",\n",
    "            'nstep': 999,\n",
    "            'tstress': False,\n",
    "            'tprnfor': True,\n",
    "            'disk_io': \"none\",\n",
    "            'etot_conv_thr': 1.0e-5,\n",
    "            'forc_conv_thr': 1.0e-5,\n",
    "            },\n",
    "        'system': {\n",
    "            'ibrav': 0,\n",
    "            'tot_charge': 0.0,\n",
    "            'ecutwfc': 40.0,\n",
    "            'ecutrho': 600,\n",
    "            'occupations': 'smearing',\n",
    "            'degauss': 0.01,\n",
    "            'smearing': 'cold',\n",
    "            'input_dft': 'pbe',\n",
    "            'nspin': 1,\n",
    "            },\n",
    "        'electrons': {\n",
    "            'electron_maxstep': 999,\n",
    "            'scf_must_converge': True,\n",
    "            'conv_thr': 1.0e-12,\n",
    "            'mixing_mode': 'local-TF',\n",
    "            'mixing_beta': 0.25,\n",
    "            'startingwfc': 'random',\n",
    "            'diagonalization': 'david',\n",
    "            },\n",
    "        },\n",
    "    'hpc': 'your_hpc_dict_1',\n",
    "    } # the hpc can/should be changed to whatever is the most appropriate hpc for each step of the workflow\n",
    "}\n",
    "\n",
    "with open('water_standard_config.yaml', 'w') as f:\n",
    "    yaml.dump(standard_water_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008909a6",
   "metadata": {},
   "source": [
    "Next, lets add some generic dictionaries for our hpc, you should **replace this with your own hpc data** and ensure such hpcs are correctly entered in you `.expyre/config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0691a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_hpc_dict = {\n",
    "  'your_hpc_dict_0': {\n",
    "    'hpc_name' : 'your_ssh_hpc_name',\n",
    "    'gpu': True, \n",
    "    'pre_cmds': [\"command to enable correct python environment on your hpc\"], # e.g. `conda activate alomancy` or `source ~/.venvs/alomancy/bin/activate`\n",
    "    'partitions': [\"hpc_partition_name\"],\n",
    "    'node_info': {\n",
    "      'ranks_per_system': 72, \n",
    "      'ranks_per_node': 72, \n",
    "      'threads_per_rank': 1, \n",
    "      'max_mem_per_node': \"60GB\", \n",
    "    },\n",
    "  },\n",
    "  'your_hpc_dict_1': {\n",
    "    'hpc_name' : 'your_ssh_hpc_name',\n",
    "    'gpu': False, \n",
    "    'pre_cmds': [\"command to enable correct python environment on your hpc\"], \n",
    "    'partitions': [\"hpc_partition_name\"],\n",
    "    'node_info': {\n",
    "      'ranks_per_system': 72, \n",
    "      'ranks_per_node': 72, \n",
    "      'threads_per_rank': 1, \n",
    "      'max_mem_per_node': \"60GB\", \n",
    "    },\n",
    "    'high_accuracy_executable_path': \"/path/to/your/quantum/espresso/bin/pw.x\",\n",
    "    'pp_path': \"/path/to/your/pseudo/potentials/directory\",\n",
    "    'pseudo_dict': {\n",
    "      'H': \"name_of_hydrogen_pp.UPF\",\n",
    "      'O': \"name_of_oxygen_pp.UPF\", \n",
    "    },\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('hpc_config.yaml', 'w') as f:\n",
    "    yaml.dump(your_hpc_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b66263",
   "metadata": {},
   "source": [
    "Now lets combine these dictionaries together sensibly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2797181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'high_accuracy_evaluation': {'hpc': {'gpu': False, 'high_accuracy_executable_path': '/path/to/your/quantum/espresso/bin/pw.x', 'hpc_name': 'your_ssh_hpc_name', 'node_info': {'max_mem_per_node': '60GB', 'ranks_per_node': 72, 'ranks_per_system': 72, 'threads_per_rank': 1}, 'partitions': ['hpc_partition_name'], 'pp_path': '/path/to/your/pseudo/potentials/directory', 'pre_cmds': ['command to enable correct python environment on your hpc'], 'pseudo_dict': {'H': 'name_of_hydrogen_pp.UPF', 'O': 'name_of_oxygen_pp.UPF'}}, 'max_time': '30m', 'name': 'qe_dft', 'qe_input_kwargs': {'control': {'disk_io': 'none', 'etot_conv_thr': 1e-05, 'forc_conv_thr': 1e-05, 'nstep': 999, 'prefix': 'qe', 'tprnfor': True, 'tstress': False, 'verbosity': 'high'}, 'electrons': {'conv_thr': 1e-12, 'diagonalization': 'david', 'electron_maxstep': 999, 'mixing_beta': 0.25, 'mixing_mode': 'local-TF', 'scf_must_converge': True, 'startingwfc': 'random'}, 'system': {'degauss': 0.01, 'ecutrho': 600, 'ecutwfc': 40.0, 'ibrav': 0, 'input_dft': 'pbe', 'nspin': 1, 'occupations': 'smearing', 'smearing': 'cold', 'tot_charge': 0.0}}}, 'mlip_committee': {'hpc': {'gpu': True, 'hpc_name': 'your_ssh_hpc_name', 'node_info': {'max_mem_per_node': '60GB', 'ranks_per_node': 72, 'ranks_per_system': 72, 'threads_per_rank': 1}, 'partitions': ['hpc_partition_name'], 'pre_cmds': ['command to enable correct python environment on your hpc']}, 'mace_fit_kwargs': {'E0s': {1: 3.21, 8: 4.6}, 'atomic_numbers': [1, 8], 'energy_key': 'REF_energy', 'forces_key': 'REF_forces'}, 'max_time': '5H', 'name': 'mace_committee', 'size_of_committee': 3}, 'structure_generation': {'desired_number_of_structures': 50, 'hpc': {'gpu': True, 'hpc_name': 'your_ssh_hpc_name', 'node_info': {'max_mem_per_node': '60GB', 'ranks_per_node': 72, 'ranks_per_system': 72, 'threads_per_rank': 1}, 'partitions': ['hpc_partition_name'], 'pre_cmds': ['command to enable correct python environment on your hpc']}, 'max_time': '10H', 'name': 'md_1200_generation', 'run_md_kwargs': {'friction': 0.002, 'steps': 20000, 'temperature': 1200, 'timestep_fs': 0.5}, 'structure_selection_kwargs': {'atom_number_range': [9, 21], 'chem_formula_list': None, 'enforce_chemical_diversity': True, 'max_number_of_concurrent_jobs': 5}}}\n"
     ]
    }
   ],
   "source": [
    "from yaml import safe_load\n",
    "\n",
    "# load jobs_dict from a YAML files\n",
    "with open(\"water_standard_config.yaml\") as f:\n",
    "    config = safe_load(f)\n",
    "\n",
    "with open(\"hpc_config.yaml\") as f:\n",
    "    hpc_config = safe_load(f)\n",
    "\n",
    "# assign hpcs to the jobs_dict\n",
    "for job in config:\n",
    "    config[job][\"hpc\"] = hpc_config[config[job][\"hpc\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2e60b",
   "metadata": {},
   "source": [
    "Finally we have prepared our inputs correctly and are ready to run our `StandardActiveLearningWorkflow` \n",
    "\n",
    "## Running `StandardActiveLearningWorkflow`\n",
    "\n",
    "Unfortunately, not everyone has access to the same HPCs so this is quite a challenge to show in this tutorial. For now we will show you what a submission script should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPC: your_ssh_hpc_name, Job: mace_committee\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'your_ssh_hpc_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m al_workflow \u001b[38;5;241m=\u001b[39m ActiveLearningStandardMACE(\n\u001b[1;32m      5\u001b[0m     initial_train_file_path\u001b[38;5;241m=\u001b[39mtrain_set_path,\n\u001b[1;32m      6\u001b[0m     initial_test_file_path\u001b[38;5;241m=\u001b[39mtest_set_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     start_loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# run the workflow\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mal_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/ALomnacy/src/alomancy/core/base_active_learning.py:108\u001b[0m, in \u001b[0;36mBaseActiveLearningWorkflow.run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Test set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_xyzs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Core AL loop steps - these methods must be implemented by subclasses\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mlip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlip_committee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAL Loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluation results: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mevaluation_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/ALomnacy/src/alomancy/core/standard_active_learning.py:43\u001b[0m, in \u001b[0;36mActiveLearningStandardMACE.train_mlip\u001b[0;34m(self, base_name, mlip_committee_job_dict)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmace_fit_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mlip_committee_job_dict:\n\u001b[1;32m     41\u001b[0m     mlip_committee_job_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmace_fit_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 43\u001b[0m \u001b[43mcommittee_remote_submitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_remote_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlip_committee_job_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_set.xyz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_set.xyz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmlip_committee_job_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_stagetwo_compiled.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m803\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize_of_committee\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlip_committee_job_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize_of_committee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmace_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlip_committee_job_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlip_committee_job_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworkdir_str\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m mae_avg_results \u001b[38;5;241m=\u001b[39m get_mace_eval_info(\n\u001b[1;32m     63\u001b[0m     mlip_committee_job_dict\u001b[38;5;241m=\u001b[39mmlip_committee_job_dict\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mae_avg_results\n",
      "File \u001b[0;32m~/repos/ALomnacy/src/alomancy/mlip/committee_remote_submitter.py:45\u001b[0m, in \u001b[0;36mcommittee_remote_submitter\u001b[0;34m(remote_info, base_name, target_file, seed, size_of_committee, function, function_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m executor \u001b[38;5;241m=\u001b[39m RemoteJobExecutor(remote_info)\n\u001b[1;32m     40\u001b[0m job_configs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     41\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed \u001b[38;5;241m+\u001b[39m i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: i, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs}}\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size_of_committee)\n\u001b[1;32m     43\u001b[0m ]\n\u001b[0;32m---> 45\u001b[0m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_and_wait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommon_output_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlip_committee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_\u001b[39;49m\u001b[38;5;132;43;01m{job_id}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m find_target_files()\n",
      "File \u001b[0;32m~/repos/ALomnacy/src/alomancy/utils/remote_job_executor.py:253\u001b[0m, in \u001b[0;36mRemoteJobExecutor.run_and_wait\u001b[0;34m(self, function, job_configs, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mConvenience method to submit, start, and wait for multiple jobs.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    Results from all jobs\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit_multiple_jobs(function, job_configs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_all_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_all_jobs(verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleanup_jobs()\n",
      "File \u001b[0;32m~/repos/ALomnacy/src/alomancy/utils/remote_job_executor.py:170\u001b[0m, in \u001b[0;36mRemoteJobExecutor.start_all_jobs\u001b[0;34m(self, **start_kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mStart all submitted jobs.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    Additional arguments for job.start()\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs:\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msys_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader_extra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader_extra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexact_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexact_fit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpartial_node\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstart_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/alomancy/lib/python3.10/site-packages/expyre/func.py:446\u001b[0m, in \u001b[0;36mExPyRe.start\u001b[0;34m(self, resources, system_name, header_extra, exact_fit, partial_node, python_cmd, force_rerun)\u001b[0m\n\u001b[1;32m    443\u001b[0m     resources \u001b[38;5;241m=\u001b[39m Resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresources)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_name \u001b[38;5;241m=\u001b[39m system_name\n\u001b[0;32m--> 446\u001b[0m system \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystems\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_expyre_pre_run_commands\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    448\u001b[0m     pre_run_commands \u001b[38;5;241m=\u001b[39m fin\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'your_ssh_hpc_name'"
     ]
    }
   ],
   "source": [
    "from alomancy.core.standard_active_learning import ActiveLearningStandardMACE\n",
    "\n",
    "# define the workflow object\n",
    "al_workflow = ActiveLearningStandardMACE(\n",
    "    initial_train_file_path=train_set_path,\n",
    "    initial_test_file_path=test_set_path,\n",
    "    jobs_dict=config,\n",
    "    number_of_al_loops=5,\n",
    "    verbose=1,\n",
    "    start_loop=0,\n",
    ")\n",
    "\n",
    "# run the workflow\n",
    "al_workflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1079deb",
   "metadata": {},
   "source": [
    "This will, of course, fail unless you have defined your HPC dictionary properly. \n",
    "\n",
    "We have provided a demonstrative output in the `results` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alomancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
